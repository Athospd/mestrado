<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width">
<meta property="og:title" content="Automatic Acoustic Identifier for Brazilian Bird Species" />






<meta name="description" content="Automatic Acoustic Identifier for Brazilian Bird Species">

<script id="pandoc-meta" type="application/json">
{"department":"Mechanic Engineering","newpage_html_class":"page-break-after","location":"São Paulo, Brazil","link-citations":"yes","committee":[{"prefix":"Prof. Dr.","name":"Paulo Hubert","position":"Faculty Advisor"},{"prefix":"Prof. Dr.","name":"Linilson Padovese","position":"Chair of Department"}],"toc-title":"Contents","lof":true,"paged-footnotes":true,"bibliography":["packages.bib","references.bib"],"date":[{"year":"2020","month":"May"}],"subtitle":"Monograph","university":"University of São Paulo","lot":true,"output":{"pagedown::thesis_paged":{"self_contained":"no","toc":true,"css":["custom.css","thesis"],"pandoc_args":"–mathjax","number_sections":"yes"}},"sign_page":false,"author":[{"name":"Athos P. Damiani"},{"name":"Prof. PhD. Paulo do C. Hubert Jr"},{"name":"Prof. PhD. Linilson Padovese"}],"abstract":"<p style=\"text-align: justify;\">\nRecently, studies on bioacoustics have been used in several areas of\nBiology, from the Systematic, through Ethology, to the Conservation of Biodiversity.\nHowever, the potential of applying these tools in the study of\nendangered species and night fauna remains severely unexplored. This project aims to develop tools and a methodology\nfor detection and analysis for night bird presence/absence data, in particular\nStrigidae family, through the development of acoustic detectors using machines\nlearning, to apply for both large databases of acoustic landscapes scanning and\nembark in monitoring equipment installed in the field, developed by the\nAcoustics and Environment of Poli-USP. Several supervised learning approaches, such as convolutional neural networks and gradient boosting machines.\n</p>\n<br><br>\n<strong>keywords:</strong> bioacoustics, ecology acoustics, landscape monitoring, signal processing, biodiversity conservation, machine learning, automatic species regocnition, neural networks, gradient boosting machine, strigidae, owls","knit":"pagedown::chrome_print","title":"Automatic Acoustic Identifier for Brazilian Bird Species","institute":"Politechnic School of Engineering"}
</script>

<title>Automatic Acoustic Identifier for Brazilian Bird Species</title>


<link href="monografia_files/paged-0.10.1/css/thesis.css" rel="stylesheet" />
<script src="monografia_files/paged-0.10.1/js/config.js"></script>
<script src="monografia_files/paged-0.10.1/js/paged.js"></script>
<script src="monografia_files/paged-0.10.1/js/hooks.js"></script>
<script src="monografia_files/kePrint-0.0.1/kePrint.js"></script>



<link rel="stylesheet" href="custom.css" type="text/css" />

</head>

<body>

<div class="running-h1-title"></div>

<div class="running-h2-title"></div>



<!--
Title Page
-->
<div class="front-page">
  
<h2 class="author">Politechnic School of Engineering</h2>
<p class="department">Department of Mechanic Engineering</p>
<br>
<br>
<br>
<br>
<br>
<div id="header" class="title-page">
<h1 class="title">Automatic Acoustic Identifier for Brazilian Bird Species</h1>
<h1 class="subtitle"><span>Monograph</span></h1>
<br>
<br>
<h2 class="author">Athos P. Damiani</h2>
<h2 class="author">Prof. PhD. Paulo do C. Hubert Jr</h2>
<h2 class="author">Prof. PhD. Linilson Padovese</h2>
</div>
<br>
<br>
<br>
<h2 class="degree"></h2>
<br>
<p class="institute">University of São Paulo<br>São Paulo, Brazil</p>
<br>
<p class="copyright">May 2020 </p>

<!--
Bellow is the signature page
if the user wants one the YAML
should be "sign_page: true" -->


<!-- Pre content section
-Example, if user want a dedication page then they include "dedication: 'some text' in the YAML"
-If you want other pages not found in this template simply add it to this section in the same way
using the ""
-->


<div class="abstract">
<h1 class="abstract">Abstract</h1>
<p style="text-align: justify;">
Recently, studies on bioacoustics have been used in several areas of
Biology, from the Systematic, through Ethology, to the Conservation of Biodiversity.
However, the potential of applying these tools in the study of
endangered species and night fauna remains severely unexplored. This project aims to develop tools and a methodology
for detection and analysis for night bird presence/absence data, in particular
Strigidae family, through the development of acoustic detectors using machines
learning, to apply for both large databases of acoustic landscapes scanning and
embark in monitoring equipment installed in the field, developed by the
Acoustics and Environment of Poli-USP. Several supervised learning approaches, such as convolutional neural networks and gradient boosting machines.
</p>
<br><br>
<strong>keywords:</strong> bioacoustics, ecology acoustics, landscape monitoring, signal processing, biodiversity conservation, machine learning, automatic species regocnition, neural networks, gradient boosting machine, strigidae, owls
</div>



</div>

<!-- This
Container for the table of content, table of tables and table of figures etc.
- See pagedown::html_paged for details on how to use.
-->

<div class="front-matter-container">

<div id="TOC" class="level1 toc front-matter">
<h1 class="toc-title">Contents</h1>
<ul>
<li><a href="#LOT">List of Tables</a></li>
<li><a href="#LOF">List of Figures</a></li>
<li><a href="#literature-review">Literature Review</a></li>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="#notes"><span class="toc-section-number">1.1</span> Notes</a></li>
</ul></li>
<li><a href="#objective"><span class="toc-section-number">2</span> Objective</a></li>
<li><a href="#the-data"><span class="toc-section-number">3</span> The Data</a><ul>
<li><a href="#trainingtesting-data"><span class="toc-section-number">3.1</span> Training/Testing data</a></li>
<li><a href="#audio-preparation"><span class="toc-section-number">3.2</span> Audio preparation</a></li>
<li><a href="#wave-file-slicing"><span class="toc-section-number">3.3</span> Wave file slicing</a></li>
<li><a href="#data-labelling"><span class="toc-section-number">3.4</span> Data Labelling</a></li>
<li><a href="#data-annotation"><span class="toc-section-number">3.5</span> Data Annotation</a></li>
</ul></li>
<li><a href="#modelling"><span class="toc-section-number">4</span> Modelling</a></li>
<li><a href="#results"><span class="toc-section-number">5</span> Results</a></li>
<li><a href="#conclusion"><span class="toc-section-number">6</span> Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

</div>

<div id="LOT" class="section level1 lot unnumbered front-matter">
<h1>List of Tables</h1>
<ul>
<li><a href="#tab:mp3-counts">3.1</a> MP3 files downloaded.</li>
</ul>
</div>
<div id="LOF" class="section level1 lof unnumbered front-matter">
<h1>List of Figures</h1>
<ul>
<li><a href="#fig:wavesurfer-example">3.1</a> Screen shot of the wavesurfer::annotator_app() .</li>
</ul>
</div>
<div id="literature-review" class="section level1 unnumbered">
<h1>Literature Review</h1>
<p>To do.</p>
</div>
<div id="introduction" class="section level1 chapter">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>Biodiversity analysis campaigns, mainly related to insects, birds, mammals and anurans, have traditionally been carried out in a “manual” manner, through the use of specialists in loco, using hearing and vision. These campaigns are time consuming, expensive, largely dependent on the experience and knowledge of each specialist, and limited in time and space. The presence or absence of a species in a certain site depends on the coincidence of both the species and the specialist in the same time and space.</p>
<p>In order to provide tools to increase the capacity for studying and analyzing biodiversity, both in terrestrial and underwater biomes, the concept of monitoring soundscapes has been used. For this, ambient acoustic recording equipment is used which, through microphones and hydrophones, capture environmental audio whether in the infrasound, audible or ultrasound bands. These devices can be autonomous in the sense that they have internal energy and storage capacity that allow them to work for a few days or several months. But they can also be permanent in the sense that they have no energy restrictions (either through the use of solar panels, or from another external energy source), with eventual transmission of information in real time. The generated database contains information that enables a great diversity of studies and analysis, such as: animal communication and behavior, landscape ecology and conservation biology, research in population dynamics (Campos-Cerqueira, 2016), communities monitoring (Dawson and Efford, 2009), detection of endemic, threatened or even unknown species (Vernier, 2012) and in studies regarding migration (Salamon et al, 2016).</p>
<p>Among these, the development of bird sound classification systems has been widely debated and reviewed in the literature, and some references consulted are listed at the end of the proposal. Several problems remain open, with the main one, perhaps, being about find ways of dealing with the immense variability of vocalizations and ambient acoustic conditions. Annually, transdisciplinary teams composed of researchers from different international organizations are mobilized to propose solutions to this type of challenge in an event called BirdCLEF<span id="fn1" class="footnote" data-pagedown-footnote-number="1" style="white-space: pre-line;"><a href="https://www.imageclef.org/node/230" class="uri">https://www.imageclef.org/node/230</a></span> , which is an important source of information about this subject.</p>
<p>Lacmam (Laboratory of Acoustics and Environment of EP–USP) has several years of experience in this activity, either in the development of its own hardware and software technology, or in conducting long-term campaigns for monitoring and analyzing soundscapes, both terrestrial and underwater (Figure 1 and Figure 2), as detailed in Methodology Session.</p>
<p>In this context, this project aims to take an additional step in innovation by developing technology for automatic detection of certain acoustic events. Of particular interest is the detection of some species of Strigiformes (owls), incorporating machine learning methods in the soundscape monitoring technology package already developed by the laboratory, in favor of the conservation of species and ecosystems. In the context of the species detection problem, there is a particularly challenging issue: detection of night birds due to the specialist’s limited sight and locomotion caused by the biome.</p>
<p>On the other hand, 24 hours persistent acoustic monitoring equipments has relatively low cost and can be cover a wide region. Therefore an installation of automated detection software on these equipments, or using these softwares for analysis of large acoustic databases, enables an efficient and effective method/permanent monitoring in large areas at the same time that it provides quality information on ecological behavior.</p>
<p>Although the proposal’s emphasis is on the automated recognition of some given bird species, the methodology to be developed may also be applied to other species of birds, anurans and, even in underwater fauna, such as marine mammals and chorus fish, among others.</p>
<p>This project was inspired by needs pointed out by managers from Federal and State Conservation Unities as well as by Environment Conservation NGOs. These are concrete demands to which we hope to contribute.</p>
<div id="notes" class="section level2">
<h2><span class="header-section-number">1.1</span> Notes</h2>
<blockquote>
<p>“There is the need for shared datasets with annotations of a wide variety of calls for a large number of species if methods that are suitable for conservation work are to be developed.”
— Automated birdsong recognition in complex acoustic environments: a review</p>
</blockquote>
</div>
</div>
<div id="objective" class="section level1">
<h1><span class="header-section-number">2</span> Objective</h1>
<p>To do.</p>
</div>
<div id="the-data" class="section level1 chapter">
<h1><span class="header-section-number">3</span> The Data</h1>
<div id="trainingtesting-data" class="section level2">
<h2><span class="header-section-number">3.1</span> Training/Testing data</h2>
<p>Two main data sources with almost ready to use labels were gathered:</p>
<ul>
<li><a href="Xeno-Canto.org">Xeno-Canto.org</a></li>
<li><a href="https://www.wikiaves.com.br/">Wikiaves.com.br</a></li>
</ul>
<p>Those websites are the biggest repository of Brazilian bird species sounds. The table <a href="#tab:mp3-counts">3.1</a> show the counts of MP3 downloaded by species.</p>
<table>
<caption><span id="tab:mp3-counts">Table 3.1: </span>MP3 files downloaded.</caption>
<thead>
<tr class="header">
<th align="left">Species</th>
<th align="right">#mp3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Megascops-choliba</td>
<td align="right">803</td>
</tr>
<tr class="even">
<td align="left">Pulsatrix-koeniswaldiana</td>
<td align="right">334</td>
</tr>
<tr class="odd">
<td align="left">Strix-hylophila</td>
<td align="right">318</td>
</tr>
<tr class="even">
<td align="left">Megascops-atricapilla</td>
<td align="right">258</td>
</tr>
<tr class="odd">
<td align="left">Glaucidium-minutissimum</td>
<td align="right">208</td>
</tr>
<tr class="even">
<td align="left">Total</td>
<td align="right">1921</td>
</tr>
</tbody>
</table>
<p>No providence was made if the same audio happened to appear at both sources.</p>
<p>The {warbleR} <span class="citation">(Araya-Salas and Smith-Vidaurre <a href="#ref-warbleR2017">2017</a>)</span> and {wikiaves} <span class="citation">(Damiani <a href="#ref-R-wikiaves">2020</a>)</span> packages were used to retrieve all the mp3 available automatically. A tutorial to reproduce the extraction can be found in this link <a href="">https://athospd.github.io/mestrado/articles/data-gathering.html</a>.</p>
</div>
<div id="audio-preparation" class="section level2">
<h2><span class="header-section-number">3.2</span> Audio preparation</h2>
<p>All the audios donwloaded from Xeno-Canto and Wikiaves comes as mp3 files. Also, they can be stereo or mono, in potentially any sample rate and unnormalized. The first step of the data preparation standardize these properties.</p>
<p>Steps to apply to every mp3 file:</p>
<ol style="list-style-type: decimal">
<li>convert MP3 to WAV;</li>
<li>stereo to mono;</li>
<li>downsample to 16kHz;</li>
<li>normalize amplitude by rescaling to integers in [-32767, 32767] (i.e. 16-bit).</li>
</ol>
</div>
<div id="wave-file-slicing" class="section level2">
<h2><span class="header-section-number">3.3</span> Wave file slicing</h2>
<p>The observations of the training set will be slices with a given length of the raw wave files. For instance, if an original wave file has duration of 55 seconds, then the slicing with interval of 1 second and no overlap will result in 55 disjoint 1 second long slices.</p>
<p>Table <a href="#tab:slice-example">3.2</a> shows the layout of the sliced dataset: the original audio file name as <code>id</code>, the start point of the slice and the end point of the slice. The final slice dataset is built by stacking the slices of all the files.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:slice-example">Table 3.2: </span>Example of a dataset of slices from <code>Glaucidium-minutissimum-1066225.wav</code> audio file.
</caption>
<thead>
<tr>
<th style="text-align:left;">
id
</th>
<th style="text-align:center;">
start
</th>
<th style="text-align:center;">
end
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Glaucidium-minutissimum-1066225
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Glaucidium-minutissimum-1066225
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
Glaucidium-minutissimum-1066225
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
3
</td>
</tr>
</tbody>
</table>
<p>The experiment tested slicing with 1 second long and with 2 seconds long, with no overlap. Table <a href="#tab:slice-counts">3.3</a> shows the total counts of slices generated after the slicing process. There is a trade-off behind the window length choice. The smaller the interval, the more samples will be created. On the other hand, less information will fit in the frame causing issues for the learner. So there are a sweet spot to be discovered and this is the reason to study both 1 and 2 seconds frame datasets.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:slice-counts">Table 3.3: </span>Slices.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Species
</th>
<th style="text-align:right;">
#mp3
</th>
<th style="text-align:right;">
#1sec slices
</th>
<th style="text-align:right;">
#2sec slices
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Megascops-choliba
</td>
<td style="text-align:right;">
803
</td>
<td style="text-align:right;">
30115
</td>
<td style="text-align:right;">
15259
</td>
</tr>
<tr>
<td style="text-align:left;">
Pulsatrix-koeniswaldiana
</td>
<td style="text-align:right;">
334
</td>
<td style="text-align:right;">
14904
</td>
<td style="text-align:right;">
7529
</td>
</tr>
<tr>
<td style="text-align:left;">
Strix-hylophila
</td>
<td style="text-align:right;">
318
</td>
<td style="text-align:right;">
10812
</td>
<td style="text-align:right;">
5485
</td>
</tr>
<tr>
<td style="text-align:left;">
Megascops-atricapilla
</td>
<td style="text-align:right;">
258
</td>
<td style="text-align:right;">
10938
</td>
<td style="text-align:right;">
5536
</td>
</tr>
<tr>
<td style="text-align:left;">
Glaucidium-minutissimum
</td>
<td style="text-align:right;">
208
</td>
<td style="text-align:right;">
7339
</td>
<td style="text-align:right;">
3725
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Total
</td>
<td style="text-align:right;font-weight: bold;">
1921
</td>
<td style="text-align:right;font-weight: bold;">
74108
</td>
<td style="text-align:right;font-weight: bold;">
37534
</td>
</tr>
</tbody>
</table>
</div>
<div id="data-labelling" class="section level2">
<h2><span class="header-section-number">3.4</span> Data Labelling</h2>
<p>Each slice can contain parts of bird songs in it or not. In this section, the annotation part is suposed to be already done. The annotation task is discussed in section <a href="#data-annotation">Data Annotation</a>.</p>
<p>The table <a href="#tab:annotation-rds">3.4</a> show an example of the tables that were generated to store where the bird songs happened in each wave file and the respective bird species that made it. At the end of the labelling process the dataset had six distinct labels: five from bird species and one for “other events than bird songs”. This later label was called as “unknown” and these other, unknown events could be silence, noise, any other animal, or even other birds different from the five ones previous picked.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:annotation-rds">Table 3.4: </span>The first four rows of the table storing the moments where the bird songs happen inside the <code>Glaucidium-minutissimum-1066225.wav</code> audio file along with the respective bird species label.
</caption>
<thead>
<tr>
<th style="text-align:left;">
audio_id
</th>
<th style="text-align:left;">
region_id
</th>
<th style="text-align:right;">
start
</th>
<th style="text-align:right;">
end
</th>
<th style="text-align:left;">
label
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Glauc…225.wav
</td>
<td style="text-align:left;">
k6109vbm8ro
</td>
<td style="text-align:right;">
0.21
</td>
<td style="text-align:right;">
1.19
</td>
<td style="text-align:left;">
Glauc…mum
</td>
</tr>
<tr>
<td style="text-align:left;">
Glauc…225.wav
</td>
<td style="text-align:left;">
upsuth6aang
</td>
<td style="text-align:right;">
6.10
</td>
<td style="text-align:right;">
7.40
</td>
<td style="text-align:left;">
Glauc…mum
</td>
</tr>
<tr>
<td style="text-align:left;">
Glauc…225.wav
</td>
<td style="text-align:left;">
cgqlbd1m4r
</td>
<td style="text-align:right;">
17.22
</td>
<td style="text-align:right;">
18.36
</td>
<td style="text-align:left;">
Glauc…mum
</td>
</tr>
<tr>
<td style="text-align:left;">
Glauc…225.wav
</td>
<td style="text-align:left;">
p97kav4lem8
</td>
<td style="text-align:right;">
23.90
</td>
<td style="text-align:right;">
24.88
</td>
<td style="text-align:left;">
Glauc…mum
</td>
</tr>
<tr>
<td style="text-align:left;">
Glauc…225.wav
</td>
<td style="text-align:left;">
ft0i9775ejg
</td>
<td style="text-align:right;">
30.36
</td>
<td style="text-align:right;">
31.29
</td>
<td style="text-align:left;">
Glauc…mum
</td>
</tr>
</tbody>
</table>
<p>After build one of such tables for every wave file, a final dataset with all the slices labeled was made, one for one second long slices and other for two second long. Those tables have 74108 and 37534 rows respectively, as shown in Table <a href="#tab:slice-counts">3.3</a>. This dataset was made by joining <a href="#tab:slice-example">3.2</a> and <a href="#tab:annotation-rds">3.4</a> by interval.
The slices id were convinientely named as</p>
<p><code>{bird-species}@{start}@{end}@.wav</code></p>
<p>so one can retrieve where this slices lays in the original wave file by separating the parts by <code>@</code>. For instance, Table <a href="#tab:db-labeled">3.5</a> show that a <em>Glaucidium minutissimum</em> sang between the seconds 0 and 1, and also between the seconds 0 and 2 of the <code>Glaucidium-minutissimum-1066225.wav</code> audio file. The seconds 10 to 12 there was no bird song events of interest, so it was labeled as “unknown”.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:db-labeled">Table 3.5: </span>The first four rows of the table of all 1 second slices along with the bird species label.
</caption>
<thead>
<tr>
<th style="text-align:left;">
audio_id
</th>
<th style="text-align:left;">
slice_id
</th>
<th style="text-align:left;">
label
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Glaucidium-minutissimum-1066225.wav
</td>
<td style="text-align:left;">
<a href="mailto:Glaucidium-minutissimum-1066225@0@1" class="email">Glaucidium-minutissimum-1066225@0@1</a>@.wav
</td>
<td style="text-align:left;">
Glaucidium-minutissimum
</td>
</tr>
<tr>
<td style="text-align:left;">
Glaucidium-minutissimum-1066225.wav
</td>
<td style="text-align:left;">
<a href="mailto:Glaucidium-minutissimum-1066225@1@2" class="email">Glaucidium-minutissimum-1066225@1@2</a>@.wav
</td>
<td style="text-align:left;">
Glaucidium-minutissimum
</td>
</tr>
<tr>
<td style="text-align:left;">
Glaucidium-minutissimum-1066225.wav
</td>
<td style="text-align:left;">
<a href="mailto:Glaucidium-minutissimum-1066225@10@11" class="email">Glaucidium-minutissimum-1066225@10@11</a>@.wav
</td>
<td style="text-align:left;">
unknown
</td>
</tr>
<tr>
<td style="text-align:left;">
Glaucidium-minutissimum-1066225.wav
</td>
<td style="text-align:left;">
<a href="mailto:Glaucidium-minutissimum-1066225@11@12" class="email">Glaucidium-minutissimum-1066225@11@12</a>@.wav
</td>
<td style="text-align:left;">
unknown
</td>
</tr>
<tr>
<td style="text-align:left;">
Glaucidium-minutissimum-1066225.wav
</td>
<td style="text-align:left;">
<a href="mailto:Glaucidium-minutissimum-1066225@12@13" class="email">Glaucidium-minutissimum-1066225@12@13</a>@.wav
</td>
<td style="text-align:left;">
unknown
</td>
</tr>
</tbody>
</table>
</div>
<div id="data-annotation" class="section level2">
<h2><span class="header-section-number">3.5</span> Data Annotation</h2>
<p>The software used to annotate the audios was the {wavesurfer} R package <span class="citation">(Damiani and Katspaugh <a href="#ref-R-wavesurfer">2020</a>)</span><span id="fn2" class="footnote" data-pagedown-footnote-number="2" style="white-space: pre-line;">The {wavesurfer} annotator app is open-source and was developed by the authors of this monograph.</span>. The parts containing bird songs were manually annotated by the humans.</p>
<div class="figure" style="text-align: center"><span id="fig:wavesurfer-example"></span>
<img src="monografia_files/figure-html/wavesurfer-example-1.png" alt="Screen shot of the wavesurfer::annotator_app()." width="232" />
<p class="caption">
Figure 3.1: Screen shot of the wavesurfer::annotator_app().
</p>
</div>
<p>Each one of the wave files were annotated manually by visual and by hearing. The software stored a dataset with the annotation information as shown in Table <a href="#tab:annotation-rds">3.4</a>. Wave files with pure silence or damaged were discarded from the analysis.</p>
</div>
</div>
<div id="modelling" class="section level1 chapter">
<h1><span class="header-section-number">4</span> Modelling</h1>
</div>
<div id="results" class="section level1">
<h1><span class="header-section-number">5</span> Results</h1>
<p>To do.</p>
</div>
<div id="conclusion" class="section level1">
<h1><span class="header-section-number">6</span> Conclusion</h1>
<p>To do.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<!-- This '<div id="refs"></div>' needs to 
be here if you have appendix pages 
otherwise you can remove it.-->
<div id="refs">
<div id="ref-warbleR2017">
<p>Araya-Salas, M., and G. Smith-Vidaurre. 2017. “WarbleR: An R Package to Streamline Analysis of Animal Acoustic Signals.” <em>Methods in Ecology and Evolution</em>. <a href="http://dx.doi.org/10.1111/2041-210X.12624">http://dx.doi.org/10.1111/2041-210X.12624</a>.</p>
</div>
<div id="ref-R-wikiaves">
<p>Damiani, Athos. 2020. <em>Wikiaves: API to Retrieve Sounds from Wikiaves.com.br</em>.</p>
</div>
<div id="ref-R-wavesurfer">
<p>Damiani, Athos, and Katspaugh. 2020. <em>Wavesurfer: Create Interactive Sound Player and Visualizer in R</em>. <a href="https://github.com/athospd/wavesurfer/issues">https://github.com/athospd/wavesurfer/issues</a>.</p>
</div>
</div>
</div>



<script>
// when the page is about to reload via servr, remember the scroll position
document.addEventListener('servr:reload', function(e) {
  sessionStorage.setItem('pagedown-scroll', window.scrollY);
});
</script>
</body>
</html>
