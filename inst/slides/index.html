<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Automated Bird Species Recognition Based on Their Songs</title>
    <meta charset="utf-8" />
    <meta name="author" content="Athos Petri Damiani" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/all.min.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="css\custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Automated Bird Species Recognition Based on Their Songs
## Project for Master Degree
### Athos Petri Damiani
### November 2020 - School of Engineering/USP

---




# About me

.pull-left[

Athos Damiani, 32, Statistician from IME-USP, Brazil.

Master Student at Poli-USP, Brazil.

&lt;hr/&gt;
Project topic:

&lt;div align="center"&gt;
&lt;strong style = "color: red"&gt;Automated Bird Species Recognition Based on Their Songs&lt;/strong&gt;
&lt;/div&gt;

.small[

- **Advisor:** PhD. Linilson Padovese

- **Co-advisor:** PhD. Paulo Hubert Jr

- Approach: Supervised Machine Learning

]
]

.pull-right[

![aplicacao_corujas](img/aplicacao_corujas3.png)

]

---

# About LACMAM


&lt;img src="img/lacman.png" width="850" /&gt;



---

# Objectives

1) Literature review of methodologies used in bird species regocgnition tasks.

2) Build an automatic detector (classifier) for 5 brazilian bird species.

3) Scan a 5TB sized soundscape to try to spot those 5 bird species in it.

# Agenda for this presentation

1) My findings and perception about my literature review

2) Some early experiments and partial results

---

# Conclusions from literature review

### 1) Deep learning methods are proving to be the best approach for bird species recognition tasks.

### 2) Annotated dataset needed.


---

# Methodologies

.pull-left[

&lt;img src="img/avianbiology.png" width="850" /&gt;

Setups:

- Wavelet Denoising
- Crops/Filters
- Data augmentation
- Resampling


]

.pull-right[

Data Representations

- Raw amplitudes
- Spectrogram
- Mel Spec
- MFCC
- DWT
- Morphological
- SIFS

Methods

- HMM
- GMM
- Random Forest
- Deep Learning
- SVM

]

---

# Evidences of deep learning success

&lt;img src="img/kaggle1.png" width="850" /&gt;

All of the top solutions used `PyTorch` for fiting the models

---

# Evidences of deep learning success

**Other audio competitions:**
 
 (ongoing)
 
&lt;img src="img/kaggle2.png" width="450" /&gt;

- [Freesound Audio Tagging](https://www.kaggle.com/c/freesound-audio-tagging-2019) - CNNs
- [Bird Challange](https://www.kaggle.com/c/the-icml-2013-bird-challenge) - CNNs
- [Whale Detection](https://www.kaggle.com/c/whale-detection-challenge) - template matching approach
- [DCase - Bird Detection](http://dcase.community/challenge2018/task-bird-audio-detection) - CNNs (Lasseck, 2018)

.footnote[
[Lasseck, Mario. 2018. ACOUSTIC BIRD DETECTION WITH DEEP CONVOLUTIONAL NEURAL NETWORKS. Berlin.](http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Lasseck_76.pdf)
]


---

# Partial Results...

.pull-left[

#### MFCC + LightGBM

```r
##           Truth
## Prediction    1    2    3
##          1 4057    4  172
##          2  187  358    1
##          3  361    1  322
```

Accuracy: 87%


#### MelSp+ResNet18 (Keydana, 2020)

```r
##           Truth
## Prediction    1    2    3
##          1 4049   20  164
##          2   79  443    0
##          3  222    1  461
```

Accuracy: 91%


]

.pull-right[

#### Raw Audio + 1D CNN (Abdoli, 2019)

```r
##           Truth
## Prediction    1    2    3
##          1 4010   24  199
##          2  102  443    1
##          3  248    3  433
```

Accuracy: 89%


#### Ideas from other papers

1) Duration of audio slices and FFT size

2) Usage of two or more algoritms ensembled (stacking)

3) Rectangular Kernels

]

---

# Discussion/Opinion

**Pros of Deep Learning**

- Automatic feature engineering and powerful performance (even when dealing with noisy audios).

- Modular, easy to share, reuse and reproduce.

- Tools shuch as [PyTorch](https://pytorch.org/) are free and "research-and-production-first".

- Designed to be fast (optimized for GPUs).

**Cons of Deep Learning**

- It is not a trivial field to initiate.

- Requires at least basic programming background.

- **Requires large amount of data!**


---

# Hand crafted vs Automatic Feature Engineering: An Illustration

&lt;img src="img/kernels.png" width="650" /&gt;

---

# Annotated dataset needed

Absence of labelled datasets is the "bottle neck".

&gt;  "It is important that benchmark datasets are available, so that different researchers can compare their methods on the same datasets, and using the same metrics."

&lt;br/&gt;

&gt; “There is the need for shared datasets with annotations of a wide variety of calls for a large number of species if methods that are suitable for conservation work are to be developed.” 

&lt;br/&gt;

**— Automated birdsong recognition in complex acoustic environments: a review (Nirosha Priyadarshani, Stephen Marsland and Isabel Castro, 2017)**


---

# Datasets "Culture" in Machine Learning

.pull-left[


&lt;img src="img/datasets.png" width="330" /&gt;

Source: Pytorch.org

]

.pull-rigth[


&lt;img src="img/datasetdownload.png" width="370" /&gt;

&lt;br/&gt;

&lt;img src="img/datasetcitation.png" width="370" /&gt;

]

---

# Datasets "Culture" in Machine Learning

.pull-left[

Sources of brazilian bird calls that are not "machine-learning-ready" yet:

- Xeno-canto.org
- Wikiaves.com.br 

Datasets of bird sounds from Kaggle are "Machine-learning-ready", but lacks of brazilian representants.

Solution is to build our own.

]

.pull-right[


&lt;img src="img/birdcallbr.png" width="250" /&gt;

**Not published yet**

]


---

# Data Gathering

## Xeno-canto 

{warbleR} R package by Marcelo Araya-Salas (2010)


```r
metadata_xc = map(bird_species, ~querxc(.x))
```

## Wikiaves

{wikiaves} R package from LACMAM (2019)


```r
metadata_wa = map(bird_species, ~querwa(.x))
```


---

# Data Gathering

.mp3 files downloaded.


Table: MP3 files downloaded.

|Species                  | #mp3|
|:------------------------|----:|
|Megascops-choliba        |  803|
|Pulsatrix-koeniswaldiana |  334|
|Strix-hylophila          |  318|
|Megascops-atricapilla    |  258|
|Glaucidium-minutissimum  |  208|
|Total                    | 1921|



---

# Side products

.pull-left[

Step-by-step tutorials with code for reproducibility

&lt;img src="img/site2.png" width="330" /&gt;

[https://athospd.github.io/mestrado/](https://athospd.github.io/mestrado/)

]

.pull-right[
R packages created: 

- `{wikiaves}` - mp3 download
- `{wavesurfer}` - annotation
- `{torchaudio}` - modeling
- `{mestrado}` - reproducibility

]





---

# References

- Padovese B., Padovese L. (2019) **Machine Learning for Identifying an Endangered Brazilian Psittacidae Species**

- Priyadarshani N. et al. (2017) **Automated birdsong recognition in complex acoustic environments: a review**

- Serra, O. et al. (2019) **Active contour-based detection of estuarine dolphin whistles in spectrogram images**

- Jawaherlalnehru, J. et al. (2019) **Music Instrument Recognition from Spectrogram Images Using Convolution Neural Network**

- Keydana, S. (2020) **Classifying images with torch**

- Sajjad, A et al. (2019) **End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network**

---

class: inverse, center, middle

# Thank you!


---

## library(wavesurfer)

Tool for audio annotation in R [<i class="fab fa-github"></i> `Athospd/wavesurfer`](http://github.com/Athospd/wavesurfer)


```r
# shiny UI

wavesurfer(
  "wavs_folder/wav_file.wav", # or .mp3
* visualization = 'spectrogram'
) %&gt;%
  ws_annotator(labels = c("birdsong", "silence", "insect")) %&gt;% 
  ws_minimap() %&gt;%
  ws_cursor()
```

![](img/saida_wavesurfer2.png)

---

## library(wavesurfer)

Tool for audio annotation in R [<i class="fab fa-github"></i> `Athospd/wavesurfer`](http://github.com/Athospd/wavesurfer)

![](img/annotator_app1.png)


---

# Predictive Modeling Methodologies

.pull-left[

### Spectrogram

&lt;img src="img/Espectrograma.jpg" width="430" /&gt;


### MFCC

&lt;img src="img/mfcc.jpg" width="430" /&gt;

]

.pull-right[

&lt;br&gt; 
**passo 1)** transform...

&lt;br&gt; 
$$
mel = 2595 \log_{10}(1 + \frac{f}{700})
$$

&lt;br&gt; 
**passo 2)** weighted mean by frequency region...

&lt;br&gt; 
&lt;img src="img/mel_filters.jpg" width="430" /&gt;

]

Fonte: [haythamfayek.com](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html)


---

# Predictive Modeling Methodologies

## Convolutional Neural Networks (CNN's)


![](img/cnn.png)


.footnote[
fonte: http://tommymullaney.com/projects/rhythm-games-neural-networks
]


---

# Predictive Modeling Methodologies

## Convolutional Neural Networks (CNN's)

.pull-left[
- Define a matrix of weights (the "shadow" onto animation)

- Scan the image with this matrix (blue matrix). 

- The new 'imagem' (in green) is made by the convolution between the blue matrix and the shadow.

]

.pull-right[
![](img/padding_strides.gif)

]

Fonte: [Conv arithmetic](https://github.com/vdumoulin/conv_arithmetic)
---

# Predictive Modeling Methodologies

## Gradient Boosting Machines

&lt;img src="img/gbm.png" width="430" /&gt;

Where x represents all of the pixels from a MFCC set of an audio sample: 
- 1 second long slices 
- FFT window of 512 samples and sample rate of 16kHz with no overlap
- 13 MFCC's

Total pixels: `\(13 \times 1 \times (16000 / 512) \approx 406\)`

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
